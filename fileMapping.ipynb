{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b894fe-9368-4383-9a53-191df825b4e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 논문 (git commit log 활용)\n",
    "## 프로젝트 git 대상 파일 목록을 메뉴별로 mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e8acf7-396d-4e6d-bc9f-b5d314f508a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install jupyterlab-kite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f04191-f337-4747-b007-0341958ac7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03638a54-6dd1-4787-926d-7bf4ee72186c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1-1. xml파일에서 .do 포함하는 파일 추출하기\n",
    "### findstr /s  action.*\\.do D:\\sanhakin\\src\\main\\*.xml > xml_do_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc5145c9-9f83-44ee-9ff0-0e326448e1cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명령어가 성공적으로 실행되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# xml_do 파일을 읽어서 정제 후 xml명과 do 포함 텍스트를 csv 파일로 저장\n",
    "command = 'findstr /s action.*\\\\.do D:\\\\sanhakin\\\\src\\\\main\\\\*.xml > ./txt/xml_do_list.txt'\n",
    "try:\n",
    "    subprocess.run(command, shell=True, check=True)\n",
    "    print(\"명령어가 성공적으로 실행되었습니다.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"오류 발생:\", e)\n",
    "\n",
    "data = []\n",
    "\n",
    "# 텍스트 파일을 한 줄씩 읽어옵니다.\n",
    "with open(\"./txt/xml_do_list.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    sch_txt = 'action=\"/sanhakin'\n",
    "\n",
    "    for line in file:\n",
    "        # 각 줄을 스페이스로 분리하여 데이터를 추출합니다.\n",
    "        parts = line.strip().split()\n",
    "        xml_url = parts[0].replace(\"D:\\\\sanhakin\\\\src\\\\main\\\\webapp\", \"\").replace(\"\\\\\", \"/\").rsplit(':', 1)[0].rstrip()\n",
    "        # xml_url = parts[0].replace(\"D:\\\\sanhakin\\\\src\\\\main\\\\webapp\", \"\").replace(\"\\\\\", \"/\")[:-1]\n",
    " \n",
    "        for part in  parts[1:]:\n",
    "            idx = part.find(sch_txt)\n",
    "            if idx > -1:\n",
    "                 do_txt = part[idx + len(sch_txt):len(part)-1]\n",
    "                 # print(\"do_txt-->\" +  do_txt)\n",
    "                 data.append({'xml_url': xml_url, 'Do_txt': do_txt})\n",
    "                 break\n",
    "\n",
    "# 데이터를 데이터프레임으로 변환합니다.\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 데이터프레임을 출력합니다.\n",
    "# print(df)\n",
    "df.to_csv('./csv/xml_do.csv', encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0095997b-444d-4b8f-bf1f-d1b9c69bb349",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1-2. xml파일에서 .xml 포함하는 파일 추출하기 (화면에서 다른 메뉴화면 call 하는 경우) //주석으로 된 줄 제외\n",
    "### findstr /s  \\/wqxml.*\\.xml D:\\sanhakin\\src\\main\\*.xml | findstr /v /i /c:\"//\" > xml_Inxml.txt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b5f775e-8ed8-4eee-9799-b3b5dfd15848",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명령어가 성공적으로 실행되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# xml_do 파일을 읽어서 정제 후 xml명과 do 포함 텍스트를 csv 파일로 저장\n",
    "command = 'findstr /s  \\/wqxml.*\\\\.xml D:\\\\sanhakin\\\\src\\main\\\\*.xml | findstr /v /i /c:\"//\" > ./txt/xml_Inxml.txt'\n",
    "try:\n",
    "    subprocess.run(command, shell=True, check=True)\n",
    "    print(\"명령어가 성공적으로 실행되었습니다.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"오류 발생:\", e)\n",
    "    \n",
    "data = []\n",
    "\n",
    "# 텍스트 파일을 한 줄씩 읽어옵니다.\n",
    "with open('./txt/xml_Inxml.txt', 'r', encoding='utf-8') as file:\n",
    "\n",
    "    for line in file:\n",
    "        # 각 줄을 스페이스로 분리하여 데이터를 추출합니다.\n",
    "        parts = line.strip().split()\n",
    "        xml_url = parts[0].replace('D:\\\\sanhakin\\\\src\\\\main\\\\webapp', '').replace('\\\\', '/').rsplit(':', 1)[0].rstrip()\n",
    "        \n",
    "        #두번째 원소부터 /wqxml/과 .xml 을 포함하는지 체크해서 문자열 추출하기\n",
    "        for part in parts[1:]:\n",
    "            if '/wqxml/' in part:\n",
    "                start_index = part.find('/wqxml/')\n",
    "                end_index = part.find('.xml', start_index)\n",
    "                \n",
    "                if start_index != -1 and end_index != -1:\n",
    "                    inXmlPath = part[start_index:end_index + 4]\n",
    "                    # print(\"inXmlPath String:\", inXmlPath)\n",
    "                    # print(\"xml_path-->\" +  xml_path)\n",
    "                    data.append({'xml_url': xml_url, 'inXml': inXmlPath})\n",
    "\n",
    "# 데이터를 데이터프레임으로 변환합니다.\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop_duplicates()\n",
    "# 데이터프레임을 출력합니다.\n",
    "# print(df)\n",
    "df.to_csv('./csv/xml_Inxml.csv', encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4270bc3d-377e-481d-a3bd-10bb6d559867",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. _Controller.java 파일에서 .do 포함 텍스트 추출\n",
    "### findstr /s  \\@RequestMapping\\(.*\\) D:\\sanhakin\\src\\main\\*Controller.java > controller_do.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4385082-5415-498a-b6de-52128eed4b31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명령어가 성공적으로 실행되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# do_controller 파일을 읽어서 정제 후 do 포함 텍스트와 controller명을 csv 파일로 저장\n",
    "command = 'findstr /s  \\@RequestMapping\\(.*\\) D:\\\\sanhakin\\\\src\\\\main\\\\*Controller.java > ./txt/controller_do.txt'\n",
    "try:\n",
    "    subprocess.run(command, shell=True, check=True)\n",
    "    print(\"명령어가 성공적으로 실행되었습니다.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"오류 발생:\", e)\n",
    "    \n",
    "# 빈 리스트를 생성하여 데이터 저장 준비\n",
    "data = []\n",
    "\n",
    "# 텍스트 파일을 한 줄씩 읽어옵니다.\n",
    "with open(\"./txt/controller_do.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        # 각 줄을 '@RequestMapping'로 분리하여 데이터를 추출합니다.\n",
    "        parts = line.split('@RequestMapping')\n",
    "        \n",
    "        if not any(part.startswith(\"D:\\\\sanhakin\\\\\") for part in parts):\n",
    "            continue\n",
    "        \n",
    "        controller_url = parts[0].replace(\"D:\\\\sanhakin\\\\\", \"\").replace(\"\\\\\", \"/\").rsplit(':', 1)[0].rstrip()\n",
    "\n",
    "        #경로와 파일이름 분리\n",
    "        controller_path, controller_file = os.path.split(controller_url)\n",
    "        controller_path = os.path.dirname(controller_path)\n",
    "        \n",
    "        # 'ctrl'로 분리해서 controller.java 경로 추출\n",
    "        # controller_path_file = controller_url.split('/ctrl/')\n",
    "        \n",
    "        idx = parts[1].find(\",\")\n",
    "        if idx >  -1:\n",
    "            for text in parts[1].split(\",\"):\n",
    "                do_txt_match = re.search(r'\"([^\"]*)\"', text)\n",
    "                do_txt = do_txt_match.group(1) if do_txt_match else None\n",
    "                data.append({'controller_url': controller_url, 'controller_path':controller_path, 'controller_file': controller_file, 'Do_txt': do_txt})\n",
    "        else:\n",
    "            do_txt_match = re.search(r'\"([^\"]*)\"', parts[1])\n",
    "            do_txt = do_txt_match.group(1) if do_txt_match else None\n",
    "            data.append({'controller_url': controller_url, 'controller_path':controller_path, 'controller_file': controller_file, 'Do_txt': do_txt})\n",
    "                \n",
    "# 데이터를 데이터프레임으로 변환합니다.\n",
    "df = pd.DataFrame(data)\n",
    "# 데이터프레임을 출력합니다.\n",
    "# print(df)\n",
    "df.to_csv('./csv/controller_do.csv', encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57ef346-85f6-4e93-900a-0ba74592af3b",
   "metadata": {},
   "source": [
    "## 3. _DAO.java 파일에서 sql_id 텍스트(\" \"로 묶인 문자열) 추출 \n",
    "### findstr /s /i /r /c:\"\\\".*\\\"\" \"D:\\sanhakin\\src\\main\\*DAO.java\" | findstr /v /i /c:\"@SuppressWarnings\" | findstr /v /i /c:\"@Repository\"  > DAO_sql.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc5cd062-788e-4cf5-9288-4e2c4e620c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명령어가 성공적으로 실행되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# DAO_sql.txt(최초 search 목록에서 eyeCheck로 일부 정제) 파일을 읽어서 정제 후 xml명과 do 포함 텍스트를 csv 파일로 저장\n",
    "# D:\\sanhakin\\src\\main\\java\\com\\wti\\admin\\bizDvr\\dao\\BudgetDvrRqstSttnDAO.java:        return (List<Map>) list(\"budgetDvrRqstSttnDAO.selectBudgetDvrRqstSttnList\", input);\n",
    "#command = r'findstr /s /i /r /c:\"\\\".*\\\"\" \"D:\\sanhakin\\src\\main\\*DAO.java\" | findstr /v /i /c:\"@SuppressWarnings\" | findstr /v /i /c:\"@Repository\" > DAO_sql.txt'\n",
    "command = (\n",
    "    r'findstr /s /i /r /c:\"\\\".*\\\"\" \"D:\\sanhakin\\src\\main\\*DAO.java\" '\n",
    "    r'| findstr /v /i /c:\"@SuppressWarnings\" '\n",
    "    r'| findstr /v /i /c:\"@Repository\" > ./txt/DAO_sql.txt'\n",
    ")\n",
    "try:\n",
    "    subprocess.run(command, shell=True, check=True)\n",
    "    print(\"명령어가 성공적으로 실행되었습니다.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"오류 발생:\", e)\n",
    "    \n",
    "data = []\n",
    "\n",
    "# 텍스트 파일을 한 줄씩 읽어옵니다.\n",
    "with open(\"./txt/DAO_sql.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        # 각 줄을 스페이스로 분리하여 데이터를 추출합니다.\n",
    "        parts = line.strip().split()\n",
    "\n",
    "        if not any(part.startswith(\"D:\\\\sanhakin\\\\\") for part in parts):\n",
    "            continue\n",
    "        \n",
    "        DAO_url = parts[0].replace(\"D:\\\\sanhakin\\\\\", \"\").replace(\"\\\\\", \"/\").rsplit(':', 1)[0].rstrip()\n",
    "\n",
    "        #경로와 파일이름 분리\n",
    "        DAO_path, DAO_file = os.path.split(DAO_url)\n",
    "        DAO_path = os.path.dirname(DAO_path)\n",
    "        ## 'dao'로 분리해서 DAO.java 경로 추출\n",
    "        # DAO_path_file = DAO_url.split('/dao/')\n",
    "        \n",
    "        # if len(DAO_path_file) == 2:\n",
    "        #     DAO_path, DAO_file = DAO_path_file\n",
    "        # else:\n",
    "        #     # Handle the case where the split didn't result in two parts\n",
    "        #     DAO_path, DAO_file = DAO_path_file[0], None\n",
    "\n",
    "        sqlID_match = re.search(r'\"([^\"]*)\"', line)\n",
    "        sqlID = sqlID_match.group(1) if sqlID_match else None\n",
    "\n",
    "        # Append data to the list\n",
    "        data.append({'DAO_url': DAO_url, 'DAO_path': DAO_path, 'DAO_file': DAO_file, 'sqlID': sqlID})\n",
    "\n",
    "# 데이터를 데이터프레임으로 변환합니다.\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 데이터프레임을 출력합니다.\n",
    "# print(df)\n",
    "df.to_csv('./csv/DAO_sqlID.csv', encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8a524a-c855-4fbc-9849-1b38f14f0826",
   "metadata": {},
   "source": [
    "## 4. _SQL.xml 파일에서 sqlID 추출 ( \"<\" ,\"id=\" ,\">\" 포함하는 문자열)\n",
    "### SQLxml_sqlID.txt  ( sqlxml - sqlID )  sqlxml_sql.txt (2차가공) --> DAO_sqlID.csv\n",
    "\n",
    "### findstr /s /r /c:\"\\<.*\\s*id\\s*\\=\\s*\\>\" D:\\sanhakin\\src\\main\\*_SQL.xml  > SQLxml_sqlID.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f188b7-db14-4011-a098-cfb672b2fddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# 텍스트 파일을 한 줄씩 읽어옵니다.\n",
    "with open(\"./txt/SQLxml_sqlID.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "\n",
    "    for line in file:\n",
    "        # 각 줄을 스페이스로 분리하여 데이터를 추출합니다.\n",
    "        parts = line.strip().split()\n",
    "        SQLxml_url = parts[0]\n",
    "        SQLxml_url = SQLxml_url.replace(\"D:\\\\sanhakin\\\\\", \"\" ).replace(\"\\\\\", \"/\").rsplit(':', 1)[0].rstrip()\n",
    "\n",
    "        for part in  parts[1:]:\n",
    "            #count = part.count('\"')\n",
    "            # 다음 문자로 시작하는 문자열은 제외\n",
    "            if (part.startswith(\"parameterClass\") or part.startswith(\"resultClass\") or part.startswith(\"refid\") or part.startswith(\"java\\.util\\.\")):\n",
    "                continue\n",
    "            else :\n",
    "                fst_idx = part.find('\"')\n",
    "                if fst_idx > -1:\n",
    "                    snd_idx = part.find('\"', fst_idx + 1)\n",
    "                    sqlID = part[fst_idx + 1:snd_idx]\n",
    "                    # print(\"SQLxml_url-->\" +  SQLxml_url + \" slqID-->\"+slqID)\n",
    "                    data.append({'SQLxml_url': SQLxml_url, 'sqlID': sqlID})\n",
    "\n",
    "# 데이터를 데이터프레임으로 변환합니다.\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 데이터프레임을 출력합니다.\n",
    "# print(df)\n",
    "df.to_csv('./csv/SQLxml_sqlID.csv', encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bac4c63-8b7a-42be-88ca-aff6fe586b1d",
   "metadata": {},
   "source": [
    "## 5. txt 파일 읽어서 csv 파일로 저장 (데이터 전처리)\n",
    "### service, serviceImpl, DAO, Controller, _SQL.xml, 화면 xml\n",
    "#### [ dir /s /b D:\\sanhakin\\src\\main\\*service.java > service.txt  ]\n",
    "#### [ dir /s /b D:\\sanhakin\\src\\main\\*serviceImpl.java > serviceImpl.txt ]\n",
    "#### [ dir /s /b D:\\sanhakin\\src\\main\\*DAO.java > DAO.txt ]\n",
    "#### [ dir /s /b D:\\sanhakin\\src\\main\\*Controller.java > Controller.txt ]\n",
    "#### [ dir /s /b D:\\sanhakin\\src\\main\\*_SQl.xml > slqXml.txt ]\n",
    "#### [ dir /s /b D:\\sanhakin\\src\\main\\weuare\\*.xml > xm.txt ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb894c7-09dd-4060-a085-f1729ffa4679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "txt_paths = ['./txt/service.txt', './txt/serviceImpl.txt', './txt/DAO.txt', './txt/Controller.txt', './txt/slqXml.txt', './txt/xml.txt']\n",
    "csv_paths = ['./csv/service.csv', './csv/serviceImpl.csv', './csv/DAO.csv', './csv/Controller.csv', './csv/slqXml.csv', './csv/xml.csv']\n",
    "file_tys = ['service', 'impl', 'dao', 'ctrl', 'sql', 'xml']\n",
    "\n",
    "for i, (txt_path, csv_path) in enumerate(zip(txt_paths, csv_paths)):\n",
    "    file_ty = file_tys[i]\n",
    "    \n",
    "    # Read the text file and write its contents to a CSV file\n",
    "    with open(txt_path, 'r') as txt_file, open(csv_path, 'w', newline='') as csv_file:\n",
    "        # Assuming your text file has lines with values separated by spaces\n",
    "        reader = csv.reader(txt_file, delimiter=' ')\n",
    "        writer = csv.writer(csv_file)\n",
    "        \n",
    "        # header = [prefix + '_path', 'java_file']\n",
    "        header = ['file_url', 'file_path', 'file_name', 'file_ty']\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        # Write data from the text file to the CSV file\n",
    "        for row in reader:\n",
    "            # sevice.java 경로 추출하기\n",
    "            java_url = row[0].replace(\"D:\\\\sanhakin\\\\\", \"\").replace('\\\\','/')\n",
    "            #경로와 파일이름 분리\n",
    "            java_path, java_file  = os.path.split(java_url)\n",
    "            java_path = os.path.dirname(java_path)\n",
    "\n",
    "            # java_path_file = java_url.split(split_txt)\n",
    "            # if len(java_path_file) == 2:\n",
    "            #     java_path, java_file = java_path_file\n",
    "            # else:\n",
    "            #     # Handle the case where the split didn't result in two parts\n",
    "            #     java_path, java_file = java_path_file[0], None\n",
    "\n",
    "            writer.writerow([java_url, java_path, java_file, file_ty])\n",
    "            \n",
    "    print(f\"Conversion from {txt_path} to {csv_path} completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ed9c3-d394-479f-a2e6-acb391b26722",
   "metadata": {},
   "source": [
    "## 5-2 관리대상 파일 하나로 합쳐서 csv 파일로 저장 --> 데이터베이스 wtc_src_file table에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d16179-a735-4f07-a1ac-4d3de5a220b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# java 파일 목록\n",
    "service_df = pd.read_csv(\"./csv/service.csv\", encoding=\"utf-8\") #251\n",
    "serviceImpl_df = pd.read_csv(\"./csv/serviceImpl.csv\", encoding=\"utf-8\") #369\n",
    "DAO_df = pd.read_csv(\"./csv/DAO.csv\", encoding=\"utf-8\") #276\n",
    "ctrl_df = pd.read_csv(\"./csv/Controller.csv\", encoding=\"utf-8\") #297\n",
    "slqXml_df = pd.read_csv(\"./csv/slqXml.csv\", encoding=\"utf-8\") #323\n",
    "xml_df = pd.read_csv(\"./csv/xml.csv\", encoding=\"utf-8\") #714\n",
    "\n",
    "src_files_df = pd.concat([service_df, serviceImpl_df, DAO_df, ctrl_df, slqXml_df, xml_df], ignore_index=True)\n",
    "\n",
    "src_files_df = src_files_df.drop_duplicates()\n",
    "src_files_df.fillna('')\n",
    "\n",
    "src_files_df.sort_values(by=['file_path','file_ty'], ascending=False)\n",
    "\n",
    "# 'file_id' 컬럼 추가하고 규칙에 따라 file_id 값 부여\n",
    "src_files_df['file_id'] = ['SRC_{:06}'.format(i+1) for i in range(len(src_files_df))]\n",
    "src_files_df.to_csv('./csv/src_files.csv', encoding=\"utf-8\", index=False) # wtc_src_file table에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32205914-1c67-4b88-8bd5-3a7b579fd3aa",
   "metadata": {},
   "source": [
    "## 6. csv 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfcacfb-7b2f-47fe-9b09-170a01c06860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# csv 파일 불러오기. 인코딩은 환경에 따라 변경 필요. 일반적으로 encoding= 'utf-8' 사용\n",
    "menuID_xml = pd.read_csv(\"./csv/menu_xml.csv\", encoding=\"euc-kr\")\n",
    "xml_Inxml = pd.read_csv(\"./csv/xml_Inxml.csv\", encoding=\"utf-8\")\n",
    "\n",
    "controller_do = pd.read_csv(\"./csv/controller_do.csv\", encoding=\"utf-8\") #'controller_url', 'Do_txt'\n",
    "xml_do = pd.read_csv(\"./csv/xml_do.csv\", encoding=\"utf-8\")               # xml_nm', 'Do_txt\n",
    "\n",
    "DAO_sqlID = pd.read_csv(\"./csv/DAO_sqlID.csv\", encoding=\"utf-8\")         # DAO_url', 'slqID\n",
    "SQLxml_sqlID = pd.read_csv(\"./csv/SQLxml_sqlID.csv\", encoding=\"utf-8\")   # SQLxml_url', 'slqID'\n",
    "\n",
    "# java 파일 목록   ['service', 'impl', 'dao', 'ctrl', 'sql', 'xml']\n",
    "src_files_df = pd.read_csv(\"./csv/src_files.csv\", encoding=\"utf-8\")\n",
    "\n",
    "ctrl_df = src_files_df[src_files_df['file_ty']=='ctrl']\n",
    "service_df = src_files_df[src_files_df['file_ty']=='service']\n",
    "serviceImpl_df = src_files_df[src_files_df['file_ty']=='impl']\n",
    "DAO_df = src_files_df[src_files_df['file_ty']=='dao']\n",
    "\n",
    "# commit 정보\n",
    "commitInfo_df = pd.read_csv(\"./csv/commit_info.csv\", encoding=\"utf-8\")\n",
    "# commit 파일 정보\n",
    "commitFile_df = pd.read_csv(\"./csv/commit_file_info.csv\", encoding=\"utf-8\")\n",
    "\n",
    "#화면 내에서 콜하는 xml 목록에서 메인xml의 menu_id를 찾아서 맵핑\n",
    "inXml_df = pd.merge(xml_Inxml, menuID_xml, on=['xml_url'], how='inner')[['menu_id','inXml']]\n",
    "inXml_df.rename(columns={'inXml': 'xml_url'}, inplace = True)\n",
    "\n",
    "# # 모든 xml - menu_id 합치기\n",
    "merge_xml_menu_df =  pd.concat([menuID_xml, inXml_df], ignore_index=True)\n",
    "merge_xml_menu_df = merge_xml_menu_df.drop_duplicates()\n",
    "merge_xml_menu_df.info()\n",
    "merge_xml_menu_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11941054-81fd-45b0-9bf8-c99d27d927a0",
   "metadata": {},
   "source": [
    "## 7. java 파일 경로 분리해서 컬럼 추가\n",
    "### txt 파일 읽어서 csv 파일로 저장할 때 분리해서 저장하도록 수정해서 이 과정은 pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad059af8-d9ad-4534-b9d7-90f2b33d6270",
   "metadata": {},
   "source": [
    "## 8-1. merge 파일별 menu_id 맵핑하기\n",
    "### menuID_xml   xml_do  controller_do   DAO_sqlID   SQLxml_sqlID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b25760-52e1-4d75-82ce-2d0d84a4eeb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# controller_do.info() #1447   controller_path   controller_file  Do_txt\n",
    "# service_df.info() #250        service_path        java_file\n",
    "# serviceImpl_df.info #368    serviceImpl_path    java_file\n",
    "# DAO_df.info #275            dao_path            java_file\n",
    "# SQLxml_sqlID.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b763819-656d-41c4-b5cd-461ab3f28b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 최종 맵핑을 위한 사전 dataFrame\n",
    "# controller RequestMap 과 xml 화면에서 submition 의 .do 와 맵핑\n",
    "controller_xml_df = pd.merge(controller_do, xml_do, on=['Do_txt'], how='inner')\n",
    "controller_xml_df = controller_xml_df.drop_duplicates(subset=['controller_url','xml_url' ])\n",
    "\n",
    "# DAO 에 있는 sqlId 와 SQL.xml 에 있는 sqlId 맵핑\n",
    "dao_sql_df = pd.merge(DAO_sqlID, SQLxml_sqlID, on=['sqlID'], how='inner')\n",
    "dao_sql_df = dao_sql_df.drop_duplicates(subset=['DAO_url','SQLxml_url' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85130951-2ed2-4f31-b9d0-c22a00ce8749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# menu_id 에 등록된 화면 xml 과 controller_xml_df의 xml 과 맵핑\n",
    "# 0. 화면속 call xml파일\n",
    "merge_xml_menu_df.info()\n",
    "\n",
    "# 1. controller java\n",
    "merge_xml_ctrl = pd.merge( controller_xml_df, merge_xml_menu_df, on='xml_url', how='left')\n",
    "merge_xml_ctrl.dropna(subset = ['menu_id'], inplace=True)\n",
    "\n",
    "# java 경로로 조인해서 menu_id 가져와서 dataFrame 만들기\n",
    "# 1-2. controller java\n",
    "ctrl_menu_df = pd.merge(ctrl_df, merge_xml_ctrl, left_on='file_path', right_on='controller_path', how='left')[['file_url', 'menu_id']]\n",
    "ctrl_menu_df.dropna(subset = ['menu_id'], inplace=True)\n",
    "ctrl_menu_df = ctrl_menu_df.drop_duplicates()\n",
    "\n",
    "# 2. service java\n",
    "service_menu_df = pd.merge(service_df, merge_xml_crtl, left_on='file_path', right_on='controller_path', how='left')[['file_url', 'menu_id']]\n",
    "service_menu_df.dropna(subset = ['menu_id'], inplace=True)\n",
    "service_menu_df = service_menu_df.drop_duplicates()\n",
    "\n",
    "# # 3. serviceImpl java\n",
    "serviceImpl_menu_df = pd.merge(serviceImpl_df, merge_xml_crtl, left_on='file_path', right_on='controller_path', how='left')[['file_url', 'menu_id']]\n",
    "serviceImpl_menu_df.dropna(subset = ['menu_id'], inplace=True)\n",
    "serviceImpl_menu_df = serviceImpl_menu_df.drop_duplicates()\n",
    "\n",
    "# # 4. DAO java\n",
    "DAO_menu_df = pd.merge(DAO_df, merge_xml_crtl, left_on='file_path', right_on='controller_path', how='left')[['file_url', 'menu_id']]\n",
    "DAO_menu_df.dropna(subset = ['menu_id'], inplace=True)\n",
    "DAO_menu_df = DAO_menu_df.drop_duplicates()\n",
    "\n",
    "# # controller 경로 -  DAO.java - sqlID - SQL.xml\n",
    "# # 5. _SQL.xml\n",
    "sql_menu_df = pd.merge(dao_sql_df, merge_xml_crtl, left_on='DAO_path', right_on='controller_path', how='left')[['DAO_url','DAO_path', 'SQLxml_url', 'menu_id']]\n",
    "sql_menu_df.dropna(subset = ['menu_id'], inplace=True)\n",
    "sql_menu_df = sql_menu_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03609de0-629d-41e8-beab-8de5c7cd7217",
   "metadata": {},
   "source": [
    "## 8-2 메뉴별 파일 합치고  src_files_df 과 merge 해서 file_id mapping하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098fbdee-679f-4d5e-a882-81040bd97c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_xml_menu_df 424 menu_id  xml_url  \n",
    "# ctrl_menu_df #1397         file_url  \tmenu_id\n",
    "# service_menu_df #1436      file_url  \tmenu_id\n",
    "# serviceImpl_menu_df #1544  file_url  \tmenu_id\n",
    "# DAO_menu_df #1408          file_url  \tmenu_id\n",
    "# sql_menu_df  #1556          DAO_url    DAO_path  SQLxml_url  menu_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf3920-5cae-488f-a22a-26a630334afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# menuID_java_df = pd.concat([ctrl_menu_df, service_menu_df, serviceImpl_menu_df, DAO_menu_df], ignore_index=True)\n",
    "menuID_java_df.descrypt\n",
    "menuID_java_df.head()\n",
    "\n",
    "# menuID_fileURL_df = pd.concat([merge_xml_menu_df, ctrl_menu_df, service_menu_df, serviceImpl_menu_df, DAO_menu_df, sql_menu_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7ebbad-4066-4280-a9bd-9c1a946d7045",
   "metadata": {},
   "source": [
    "## [ 최종 ] 메뉴 ID 에 해당하는 파일목록과 commit log 파일 목록 join 해서 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98299d6-0c64-42ea-8036-3f3771790af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# csv 파일 불러오기. 인코딩은 환경에 따라 변경 필요. 일반적으로 encoding= 'utf-8' 사용\n",
    "sanhakin_file_df = pd.read_csv(\"./csv/sanhakin_file.csv\", encoding=\"utf-8\") #java_url file_ty   menu_id   \n",
    "commit_info_df = pd.read_csv(\"./csv/commit_info.csv\", encoding=\"utf-8\") \n",
    "commit_file_info_df = pd.read_csv(\"./csv/commit_file_info.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039282bc-f801-4975-b44c-07215ae8a6ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sanhakin_file_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c03eb5-7b1a-4766-a0da-9384e9fb941a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', None)\n",
    "# 찾기\n",
    "# search_term = 'src/main/resources/egovframework/sqlmap/com/usr/aply/idtcag/H3gReqCompInfo_SQL.xml'\n",
    "# result_df = sanhakin_file_df[sanhakin_file_df['java_url'].str.contains(search_term)]\n",
    "\n",
    "# Select rows from commit_file_info where 'file_path' matches the condition\n",
    "menu_selected_files = commit_file_info_df[commit_file_info_df['file_path'].isin(sanhakin_file_df[sanhakin_file_df['menu_id'] == 'MENU0000000160']['java_url'])]\n",
    "selected_commit = commit_info_df[commit_info_df['commit_id'].isin(menu_selected_files['commit_id'])]\n",
    "selected_commit.sort_values(by='commit_date', ascending=False)\n",
    "selected_commit.fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc068286-1926-4b03-875a-34303d465169",
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_file_info_df[commit_file_info_df['commit_id'] == 'c337f5c43' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af1a2a-04c5-4e0d-8a65-6981265c59c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_df = pd.merge( commit_info_df, commit_file_info_df, on='commit_id', how = 'inner')[['commit_id','commit_date','commit_cls','commit_summ','file_path']]\n",
    "menu_merge_df = pd.merge( merge_df, sanhakin_file_df, left_on='file_path', right_on = 'java_url',how='inner')[['commit_id','commit_date','menu_id','menu_title','commit_cls','commit_summ','file_path']]\n",
    "menu_merge_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa0b6a-2abc-4d4b-ba26-984cc82b8b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menu_merge_df.groupby(['menu_id','menu_title']).count()\n",
    "menu_merge_df['commit_date'] = pd.to_datetime(menu_merge_df['commit_date'])\n",
    "# # Extract year and month into new columns\n",
    "menu_merge_df['commit_year'] = menu_merge_df['commit_date'].dt.year\n",
    "menu_merge_df['commit_month'] = menu_merge_df['commit_date'].dt.month\n",
    "\n",
    "# grouped_df = menu_merge_df.groupby(['commit_year', 'commit_month','menu_id','menu_title']).agg(commit_id_count=('commit_id', 'nunique')).reset_index()\n",
    "grouped_df = menu_merge_df.groupby(['commit_year', 'commit_month','commit_cls']).agg(commit_id_count=('commit_id', 'nunique')).reset_index()\n",
    "\n",
    "# menu_merge_df[['cyl','wt']].groupby(['commit_year', 'commit_month','commit_cls']).count()\n",
    "# Display the result or use it as needed\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db9e1b-a7a2-40fd-ba3b-7e8929aec40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 공부 참고\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "controller_url = \"some/path/to/your/directory/your_file.txt\"\n",
    "\n",
    "# Split the path into the directory and the file\n",
    "controller_path, controller_file = os.path.split(controller_url)\n",
    "\n",
    "# Get the directory path excluding the last directory name\n",
    "directory_path_excluding_last = os.path.dirname(controller_path)\n",
    "\n",
    "# Display the result\n",
    "print(\"Directory Path Excluding Last Directory:\", directory_path_excluding_last)\n",
    "\n",
    "# 중복 체크 및 삭제---------------------\n",
    "# Assuming df is your DataFrame and you want to check duplicates based on columns 'col1' and 'col2'\n",
    "df = pd.DataFrame({\n",
    "    'col1': [1, 2, 3, 4, 1],\n",
    "    'col2': ['A', 'B', 'C', 'D', 'A'],\n",
    "    'col3': ['X', 'Y', 'Z', 'W', 'X']\n",
    "})\n",
    "\n",
    "# Identify duplicate rows based on 'col1' and 'col2'\n",
    "duplicates = df.duplicated(subset=['col1', 'col2'], keep=False)\n",
    "\n",
    "# Display the duplicate rows\n",
    "print(df[duplicates])\n",
    "\n",
    "# Remove duplicate rows based on 'col1' and 'col2'\n",
    "df_no_duplicates = df.drop_duplicates(subset=['col1', 'col2'])\n",
    "\n",
    "# Display the DataFrame without duplicates\n",
    "print(df_no_duplicates)\n",
    "\n",
    "\n",
    "## 중복체크\n",
    "duplicate_values = DAO_sqlID[DAO_sqlID.duplicated(subset='sqlID', keep=False)]\n",
    "print(\"DataFrame with Duplicate Values:\")\n",
    "print(duplicate_values)\n",
    "## null 값 삭제\n",
    "controller_do.dropna(subset=['controller_file'], inplace=True)\n",
    "controller_do.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
